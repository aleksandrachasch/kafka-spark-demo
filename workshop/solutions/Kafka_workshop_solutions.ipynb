{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349b4601-d5bd-41da-8e19-05d1d6040945",
   "metadata": {},
   "source": [
    "# Воркшоп по Kafka и Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb0c046-4db2-41c6-8483-1c259f4efe10",
   "metadata": {},
   "source": [
    "Есть бэкенд система, которая обрабатывает покупки. Эта система в режиме реального времени отправляет данные в топик в Kafka в формате JSON. Например:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"InvoiceNo\":\"536365\",\n",
    "   \"StockCode\":\"85123A\",\n",
    "   \"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\",\n",
    "   \"Quantity\":\"6\",\n",
    "   \"InvoiceDate\":\"12/1/2010 8:26\",\n",
    "   \"UnitPrice\":\"2.55\",\n",
    "   \"CustomerID\":\"17850\",\n",
    "   \"Country\":\"United Kingdom\"\n",
    "}\n",
    "```\n",
    "\n",
    "Есть запрос создать страницу в личном кабинете каждого клиента, где бы отображалась вся история его покупок. Допустим, что эти данные будут поступать на фронтенд через API.\n",
    "\n",
    "Задача - написать пайплайн, который в потоковом режиме будет преобразовывать сообщения о покупках таким образом, чтобы API смог забрать данные по каждому клиенту с актуальным списком покупок. \n",
    "\n",
    "Базовый стек: \n",
    "- Spark Structured Streaming\n",
    "- MongoDB\n",
    "\n",
    "Можно добавить любую другую технологию или базу данных, если это необходимо. Цель - сделать так, чтобы данные о покупке поступали как можно скорее на API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd50b7-22e6-4054-be6b-d65b2406bd82",
   "metadata": {},
   "source": [
    "## Задание 0\n",
    "\n",
    "Все логи по умолчанию пишутся в консоль. Чтобы увидеть их в ноутбуке, необходимо выполнить следующие действия:\n",
    " - В консоли докера с `pyspark` выполнить команду `ipython profile create`;\n",
    " - В файле `.ipython/profile_default/ipython_kernel_config.py` раскомментировать строку `c.IPKernelApp.capture_fd_output = True`;\n",
    " - Перезапустить `kernel` в ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e80c44-db7d-42af-b202-8c809ac68d9d",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "Спроектировать пайплан. Можно нарисовать схему с базами данных, топиками Kafka и процессами Spark. Также можно опустить часть того, каким образом данные отправляются через API на фронтенд - это сейчас не так важно. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d190d7f-2f8e-4d76-8a93-f83e55fb4511",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "Подключиться к топику с помощью `Spark DataFrameStreamReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce99ea0e-9fd6-4b07-82ba-72e7f92617f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.streaming.listener import StreamingListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652ae990-ef47-4bc6-a052-cf7febf016cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a156b88b-7e1f-4885-a16a-54245b58cc96;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2692ms :: artifacts dl 82ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   1   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a156b88b-7e1f-4885-a16a-54245b58cc96\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/16ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 13:04:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 13:04:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('yp-kafka-workshop') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6dc6f-1b18-4826-af58-71bad9d73b61",
   "metadata": {},
   "source": [
    "Настройка `ReadStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994afc1c-09d0-491c-96e3-359100a6e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_user = 'de-student'\n",
    "kafka_pass = 'ltcneltyn'\n",
    "topic_name = 'yp.workshop.kafka.retail_data'\n",
    "\n",
    "df_retail = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'rc1b-2erh7b35n4j4v869.mdb.yandexcloud.net:9091') \\\n",
    "    .option('kafka.security.protocol', 'SASL_SSL') \\\n",
    "    .option('kafka.sasl.jaas.config', f'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{kafka_user}\" password=\"{kafka_pass}\";') \\\n",
    "    .option('kafka.partition.assignment.strategy', 'org.apache.kafka.clients.consumer.RoundRobinAssignor') \\\n",
    "    .option('kafka.sasl.mechanism', 'SCRAM-SHA-512') \\\n",
    "    .option('kafka.ssl.truststore.location', '/usr/lib/jvm/java-17-openjdk-amd64/lib/security/cacerts') \\\n",
    "    .option('kafka.ssl.truststore.password', 'changeit') \\\n",
    "    .option('maxOffsetsPerTrigger', \"100\") \\\n",
    "    .option('subscribe', topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f64a3-8fd8-4d9b-9400-e20d298869c3",
   "metadata": {},
   "source": [
    "Проверяем загрузку данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "014fecbe-a178-412e-a27f-6c27576d07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 13:04:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9dbd6519-8ebc-49ed-85dc-818ff9a0a632. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/10/14 13:04:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "|{\"InvoiceNo\":\"536...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleQuery = df_retail.selectExpr(\"CAST(value AS STRING)\").writeStream.format(\"console\").start()\n",
    "sampleQuery.awaitTermination(7)\n",
    "sampleQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f843284-085e-4a5a-a560-4bf90eeac88f",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "Написать непосредственно преобразование данных. Это преобразование будет выполняться в функции `foreachBatch`:\n",
    "  - Парсинг JSON. Для этого необходима схема сообщения во формате `StructType`;\n",
    "  - Фильтрация, group by, сортировка;\n",
    "  - Запись в базу данных, файл;\n",
    "  - ...\n",
    "  \n",
    "Также необходимо выбрать один из триггеров: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.trigger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c001524-d729-4d15-8a1c-787b936aea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема данных\n",
    "retail_schema = StructType([ \\\n",
    "    StructField('InvoiceNo',StringType(),True), \\\n",
    "    StructField('StockCode',StringType(),True), \\\n",
    "    StructField('Description',StringType(),True), \\\n",
    "    StructField('Quantity', IntegerType(), True), \\\n",
    "    StructField('InvoiceDate', StringType(), True), \\\n",
    "    StructField('UnitPrice', StringType(), True), \\\n",
    "    StructField('CustomerID', StringType(), True), \\\n",
    "    StructField('Country', StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21ebf-46cc-49d3-9825-e20732e30aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_config = {\n",
    "    \"connection.uri\": \"mongodb://mongodb:27017/\",\n",
    "    \"database\": \"my_database\",\n",
    "    \"collection\": \"retail_data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf667a-416d-4b91-a8c6-a4b46c6b2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, которая будет выполняться в forEachBatch\n",
    "def process_retail_data(batch_df, batch_id):\n",
    "    print(batch_df.count())\n",
    "    \"\"\" \n",
    "      Написать логику здесь:\n",
    "          1. Десереализация столбца value\n",
    "          2. Парсинг строк JSON в схему Spark\n",
    "          3. Группируем строки по CustomerID, Country\n",
    "          4. Аггрегация, где собираем все покупки одного клиента в один список\n",
    "    \"\"\"\n",
    "    res = batch_df \\\n",
    "      .select(F.col('value').cast('string')) \\\n",
    "      .select(F.from_json(F.col('value'), retail_schema).alias('ParsedValue')) \\\n",
    "      .select(F.col('ParsedValue.*')) \\\n",
    "      .groupBy('CustomerID', 'Country') \\\n",
    "      .agg(F.collect_list(F.struct('Description', 'Quantity', 'InvoiceDate')).alias('PurchaseDescription')) \n",
    "    \n",
    "    # Запись в Mongo с помощью MongoSpark\n",
    "    res.write \\\n",
    "      .format(\"mongodb\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .options(**mongo_config) \\\n",
    "      .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225fa9d-7f7d-4637-a4dc-4bcb6eece3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "  Непосредственно обработка потока данных:\n",
    "    1. Определяем папку checkpoints, куда Spark будет записывать свой прогреcc\n",
    "    2. Добавляем функцию в foreachBatch\n",
    "\"\"\"\n",
    "retail_query = df_retail \\\n",
    "  .writeStream \\\n",
    "  .option(\"checkpointLocation\", \"file:///home/jovyan/checkpoints/\") \\\n",
    "  .foreachBatch(process_retail_data) \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b992d-f7c7-4815-af6c-015feba08933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Остановить обработку:\n",
    "retail_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da8b4cb-8a5b-4a93-b427-e4b1b22c7064",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
