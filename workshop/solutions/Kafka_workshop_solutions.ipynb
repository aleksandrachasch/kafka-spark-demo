{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349b4601-d5bd-41da-8e19-05d1d6040945",
   "metadata": {},
   "source": [
    "# Воркшоп по Kafka и Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229849f-3a60-4fac-ad68-b557b937118e",
   "metadata": {},
   "source": [
    "Есть бэкенд система, которая обрабатывает покупки. Эта система в режиме реального времени отправляет данные в топик Kafka `yp.workshop.kafka.retail_data` в формате JSON. Например:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"InvoiceNo\":\"536365\",\n",
    "   \"StockCode\":\"85123A\",\n",
    "   \"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\",\n",
    "   \"Quantity\":\"6\",\n",
    "   \"InvoiceDate\":\"12/1/2010 8:26\",\n",
    "   \"UnitPrice\":\"2.55\",\n",
    "   \"CustomerID\":\"17850\",\n",
    "   \"Country\":\"United Kingdom\"\n",
    "}\n",
    "```\n",
    "\n",
    "Есть другая бэкенд система, которая обрабатывает данные пользователей. Эта система также в режиме реального времени отправляет данные в топик Kafka `yp.workshop.kafka.customer_data` в формате JSON. Например:\n",
    "\n",
    "```json\n",
    "{\n",
    "   \"CustomerID\":\"12346\",\n",
    "   \"Address\":\"Unit 1047 Box 4089\\nDPO AA 57348\",\n",
    "   \"Birthdate\":\"1994-02-20 00:46:27\",\n",
    "   \"Email\":\"cooperalexis@hotmail.com\",\n",
    "   \"Name\":\"Lindsay Cowan\",\n",
    "   \"Username\":\"valenciajennifer\"\n",
    "}\n",
    "```\n",
    "\n",
    "Данные и в том, и в другом случае отправляются при **изменении или создании**.\n",
    "\n",
    "Есть запрос создать страницу в личном кабинете каждого клиента, где бы отображалась вся история его покупок. Допустим, что эти данные будут поступать на фронтенд через API.\n",
    "\n",
    "Задача - написать пайплайн, который в потоковом режиме будет преобразовывать сообщения о покупках таким образом, чтобы API смог забрать данные по каждому клиенту с актуальным списком покупок. \n",
    "\n",
    "Базовый стек: \n",
    "- Spark Structured Streaming\n",
    "- MongoDB\n",
    "\n",
    "Можно добавить любую другую технологию или базу данных, если это необходимо. Цель - сделать так, чтобы данные о покупке поступали как можно скорее на API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd50b7-22e6-4054-be6b-d65b2406bd82",
   "metadata": {},
   "source": [
    "## Задание 0\n",
    "\n",
    "Все логи по умолчанию пишутся в консоль. Чтобы увидеть их в ноутбуке, необходимо выполнить следующие действия:\n",
    " - В консоли докера с `pyspark` выполнить команду `ipython profile create`;\n",
    " - В файле `.ipython/profile_default/ipython_kernel_config.py` раскомментировать строку `c.IPKernelApp.capture_fd_output = True`;\n",
    " - Перезапустить `kernel` в ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e80c44-db7d-42af-b202-8c809ac68d9d",
   "metadata": {},
   "source": [
    "## Задание 1\n",
    "\n",
    "Спроектировать пайплан. Можно нарисовать схему с базами данных, топиками Kafka и процессами Spark. Также можно опустить часть того, каким образом данные отправляются через API на фронтенд - это сейчас не так важно. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d190d7f-2f8e-4d76-8a93-f83e55fb4511",
   "metadata": {},
   "source": [
    "## Задание 2\n",
    "\n",
    "Подключиться к топику с помощью `Spark DataFrameStreamReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce99ea0e-9fd6-4b07-82ba-72e7f92617f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.streaming.listener import StreamingListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "652ae990-ef47-4bc6-a052-cf7febf016cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.3.0-bin-hadoop3/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6cb5f948-6577-45c4-abc1-5d0478bb670f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector;10.0.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.5.1 in central\n",
      "\t[4.5.1] org.mongodb#mongodb-driver-sync;[4.5.0,4.5.99)\n",
      "\tfound org.mongodb#bson;4.5.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.5.1 in central\n",
      ":: resolution report :: resolve 2148ms :: artifacts dl 143ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#bson;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.5.1 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.5.1 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector;10.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   1   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6cb5f948-6577-45c4-abc1-5d0478bb670f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/35ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:50:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('yp-kafka-workshop') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6dc6f-1b18-4826-af58-71bad9d73b61",
   "metadata": {},
   "source": [
    "Настройка `ReadStream`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fe6997-91b0-477b-acec-2fce4e1fa016",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_user = 'de-student'\n",
    "kafka_pass = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04eb868c-9ca6-4e02-947c-df7a8b06c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name_retail = 'yp.workshop.kafka.retail_data'\n",
    "\n",
    "df_retail = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'rc1b-2erh7b35n4j4v869.mdb.yandexcloud.net:9091') \\\n",
    "    .option('kafka.security.protocol', 'SASL_SSL') \\\n",
    "    .option('kafka.sasl.jaas.config', f'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{kafka_user}\" password=\"{kafka_pass}\";') \\\n",
    "    .option('kafka.partition.assignment.strategy', 'org.apache.kafka.clients.consumer.RoundRobinAssignor') \\\n",
    "    .option('kafka.sasl.mechanism', 'SCRAM-SHA-512') \\\n",
    "    .option('kafka.ssl.truststore.location', '/usr/lib/jvm/java-17-openjdk-amd64/lib/security/cacerts') \\\n",
    "    .option('kafka.ssl.truststore.password', 'changeit') \\\n",
    "    .option('maxOffsetsPerTrigger', \"100\") \\\n",
    "    .option('subscribe', topic_name_retail) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0132c186-6372-4c2c-a41f-efc60c3729aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name_customer = 'yp.workshop.kafka.customer_data'\n",
    "\n",
    "df_customer = spark.readStream \\\n",
    "    .format('kafka') \\\n",
    "    .option('kafka.bootstrap.servers', 'rc1b-2erh7b35n4j4v869.mdb.yandexcloud.net:9091') \\\n",
    "    .option('kafka.security.protocol', 'SASL_SSL') \\\n",
    "    .option('kafka.sasl.jaas.config', f'org.apache.kafka.common.security.scram.ScramLoginModule required username=\"{kafka_user}\" password=\"{kafka_pass}\";') \\\n",
    "    .option('kafka.partition.assignment.strategy', 'org.apache.kafka.clients.consumer.RoundRobinAssignor') \\\n",
    "    .option('kafka.sasl.mechanism', 'SCRAM-SHA-512') \\\n",
    "    .option('kafka.ssl.truststore.location', '/usr/lib/jvm/java-17-openjdk-amd64/lib/security/cacerts') \\\n",
    "    .option('kafka.ssl.truststore.password', 'changeit') \\\n",
    "    .option('maxOffsetsPerTrigger', \"100\") \\\n",
    "    .option('subscribe', topic_name_customer) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f64a3-8fd8-4d9b-9400-e20d298869c3",
   "metadata": {},
   "source": [
    "Проверяем загрузку данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "014fecbe-a178-412e-a27f-6c27576d07ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:51:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-209d3c3e-9c48-4311-9c63-67e97f656c2f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/01 11:51:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"C54...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "|{\"InvoiceNo\":\"537...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"556...|\n",
      "|{\"InvoiceNo\":\"556...|\n",
      "|{\"InvoiceNo\":\"556...|\n",
      "|{\"InvoiceNo\":\"556...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "|{\"InvoiceNo\":\"562...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"539...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"548...|\n",
      "|{\"InvoiceNo\":\"548...|\n",
      "|{\"InvoiceNo\":\"548...|\n",
      "|{\"InvoiceNo\":\"548...|\n",
      "|{\"InvoiceNo\":\"548...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"577...|\n",
      "|{\"InvoiceNo\":\"577...|\n",
      "|{\"InvoiceNo\":\"577...|\n",
      "|{\"InvoiceNo\":\"577...|\n",
      "|{\"InvoiceNo\":\"577...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"553...|\n",
      "|{\"InvoiceNo\":\"553...|\n",
      "|{\"InvoiceNo\":\"553...|\n",
      "|{\"InvoiceNo\":\"553...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "|{\"InvoiceNo\":\"550...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"541...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "|{\"InvoiceNo\":\"549...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "|{\"InvoiceNo\":\"543...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "|{\"InvoiceNo\":\"555...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 10\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "|{\"InvoiceNo\":\"571...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 11\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"544...|\n",
      "|{\"InvoiceNo\":\"C54...|\n",
      "|{\"InvoiceNo\":\"C54...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "|{\"InvoiceNo\":\"551...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 12\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "|{\"InvoiceNo\":\"568...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 13\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"574...|\n",
      "|{\"InvoiceNo\":\"C57...|\n",
      "|{\"InvoiceNo\":\"C57...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 14\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"563...|\n",
      "|{\"InvoiceNo\":\"567...|\n",
      "|{\"InvoiceNo\":\"567...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "|{\"InvoiceNo\":\"573...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "22/11/01 11:51:24 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@44f82fb5 is aborting.\n",
      "22/11/01 11:51:24 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@44f82fb5 aborted.\n",
      "22/11/01 11:51:24 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/01 11:51:24 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 16, attempt 0, stage 16.0)\n",
      "22/11/01 11:51:24 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 16, attempt 0, stage 16.0)\n",
      "22/11/01 11:51:24 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 16) (pyspark executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "sampleQuery = df_retail.selectExpr(\"CAST(value AS STRING)\").writeStream.format(\"console\").start()\n",
    "sampleQuery.awaitTermination(10)\n",
    "sampleQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc47cdba-d349-4b6b-ae8f-371433c7c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:51:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a5322d9e-554c-4c82-b57e-8479d39e9638. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/11/01 11:51:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "|{\"CustomerID\":\"12...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "22/11/01 11:51:36 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@b98ca09 is aborting.\n",
      "22/11/01 11:51:36 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@b98ca09 aborted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 11:51:36 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:480)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:381)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/11/01 11:51:36 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 22, attempt 0, stage 22.0)\n",
      "22/11/01 11:51:36 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 22, attempt 0, stage 22.0)\n",
      "22/11/01 11:51:36 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 22) (pyspark executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "sampleQuery = df_customer.selectExpr(\"CAST(value AS STRING)\").writeStream.format(\"console\").start()\n",
    "sampleQuery.awaitTermination(7)\n",
    "sampleQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f843284-085e-4a5a-a560-4bf90eeac88f",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "Написать непосредственно преобразование данных. Это преобразование будет выполняться в функции `foreachBatch`:\n",
    "  - Парсинг JSON. Для этого необходима схема сообщения во формате `StructType`;\n",
    "  - Фильтрация, group by, сортировка;\n",
    "  - Запись в базу данных, файл;\n",
    "  - ...\n",
    "  \n",
    "Также необходимо выбрать один из триггеров: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.trigger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c001524-d729-4d15-8a1c-787b936aea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема данных retail\n",
    "retail_schema = StructType([ \\\n",
    "    StructField('InvoiceNo',StringType(),True), \\\n",
    "    StructField('StockCode',StringType(),True), \\\n",
    "    StructField('Description',StringType(),True), \\\n",
    "    StructField('Quantity', IntegerType(), True), \\\n",
    "    StructField('InvoiceDate', StringType(), True), \\\n",
    "    StructField('UnitPrice', StringType(), True), \\\n",
    "    StructField('CustomerID', StringType(), True), \\\n",
    "    StructField('Country', StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39c0b76d-55bd-45d7-89a6-1783ab75fc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Схема данных customer\n",
    "customer_schema = StructType([ \\\n",
    "    StructField('CustomerID',StringType(),True), \\\n",
    "    StructField('Address',StringType(),True), \\\n",
    "    StructField('Birthdate',StringType(),True), \\\n",
    "    StructField('Email', IntegerType(), True), \\\n",
    "    StructField('Name', StringType(), True), \\\n",
    "    StructField('Username', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6de21ebf-46cc-49d3-9825-e20732e30aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mongo_config = {\n",
    "    \"connection.uri\": \"mongodb://mongodb:27017/\",\n",
    "    \"database\": \"my_database\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1cf667a-416d-4b91-a8c6-a4b46c6b2f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, которая будет выполняться в forEachBatch\n",
    "def process_retail_data(batch_df, batch_id):\n",
    "    print(batch_df.count())\n",
    "    \"\"\" \n",
    "      Написать логику здесь:\n",
    "          1. Десереализация столбца value\n",
    "          2. Парсинг строк JSON в схему Spark\n",
    "          3. Группируем строки по CustomerID, Country\n",
    "          4. Аггрегация, где собираем все покупки одного клиента в один список\n",
    "    \"\"\"\n",
    "    res = batch_df \\\n",
    "      .select(F.col('value').cast('string')) \\\n",
    "      .select(F.from_json(F.col('value'), retail_schema).alias('ParsedValue')) \\\n",
    "      .select(F.col('ParsedValue.*')) \\\n",
    "      .groupBy('CustomerID', 'Country') \\\n",
    "      .agg(F.collect_list(F.struct('Description', 'Quantity', 'InvoiceDate')).alias('PurchaseDescription')) \n",
    "    \n",
    "    # Запись в Mongo с помощью MongoSpark\n",
    "    res.write \\\n",
    "      .format(\"mongodb\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .option(\"collection\", \"retail_data\") \\\n",
    "      .options(**mongo_config) \\\n",
    "      .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4acafb58-e8fc-4d3b-90a8-c9db1be2a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция, которая будет выполняться в forEachBatch\n",
    "# Для Customer просто пишем строки в коллекцию customer_data\n",
    "def process_customer_data(batch_df, batch_id):\n",
    "    print(batch_df.count())\n",
    "    \"\"\" \n",
    "      Написать логику здесь:\n",
    "          1. Десереализация столбца value\n",
    "          2. Парсинг строк JSON в схему Spark\n",
    "    \"\"\"\n",
    "    res = batch_df \\\n",
    "      .select(F.col('value').cast('string')) \\\n",
    "      .select(F.from_json(F.col('value'), customer_schema).alias('ParsedValue')) \\\n",
    "      .select(F.col('ParsedValue.*')) \\\n",
    "    \n",
    "    # Запись в Mongo с помощью MongoSpark\n",
    "    res.write \\\n",
    "      .format(\"mongodb\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .option(\"collection\", \"customer_data\") \\\n",
    "      .options(**mongo_config) \\\n",
    "      .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e225fa9d-7f7d-4637-a4dc-4bcb6eece3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:04:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "  Непосредственно обработка потока данных:\n",
    "    1. Определяем папку checkpoints, куда Spark будет записывать свой прогреcc\n",
    "    2. Добавляем функцию в foreachBatch\n",
    "\"\"\"\n",
    "retail_query = df_retail \\\n",
    "  .writeStream \\\n",
    "  .option(\"checkpointLocation\", \"file:///home/jovyan/checkpoints/retail_query\") \\\n",
    "  .foreachBatch(process_retail_data) \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "485504be-517a-45da-b443-2d8c93c1da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "22/11/01 12:04:18 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "7\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "То же самое для customer\n",
    "\"\"\"\n",
    "customer_query = df_customer \\\n",
    "  .writeStream \\\n",
    "  .option(\"checkpointLocation\", \"file:///home/jovyan/checkpoints/customer_query\") \\\n",
    "  .foreachBatch(process_customer_data) \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "630b992d-f7c7-4815-af6c-015feba08933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 12:05:23 WARN TaskSetManager: Lost task 0.0 in stage 371.0 (TID 15692) (pyspark executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# Остановить обработку retail:\n",
    "retail_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3da8b4cb-8a5b-4a93-b427-e4b1b22c7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Остановить обработку customer:\n",
    "customer_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb26753-079f-4fe6-af8c-0f8b3b748369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
